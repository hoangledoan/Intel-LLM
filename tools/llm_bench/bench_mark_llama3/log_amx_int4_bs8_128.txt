<frozen importlib.util>:208: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.
[ INFO ] ==SUCCESS FOUND==: use_case: text_gen, model_type: llama
[ INFO ] OV Config={'INFERENCE_NUM_THREADS': 160, 'CACHE_DIR': ''}
[ WARNING ] It is recommended to set the environment variable OMP_WAIT_POLICY to PASSIVE, so that OpenVINO inference can use all CPU resources without waiting.
[ INFO ] The num_beams is 1, update Torch thread num from 80 to 16, avoid to use the CPU cores for OpenVINO inference.
[ INFO ] Model path=models/llama-3-8b-int4, openvino runtime version: 2025.0.0-17942-1f68be9f594-releases/2025/0
[ INFO ] Selected OpenVINO GenAI for benchmarking
[ INFO ] Pipeline initialization time: 3.93s
[ INFO ] Numbeams: 1, benchmarking iter nums(exclude warm-up): 3, prompt nums: 1, prompt idx: [0]
[ INFO ] [warm-up][P0] Input text: It's done, and submitted. You can play 'Survival of the Tastiest' on Android, and on the web. Playing on the web works, but you have to simulate multiple touch for table moving and that can be a bit confusing. There is a lot I'd like to talk about. I will go through every topic, insted of making the typical what went right/wrong list. Concept Working over the theme was probably one of the hardest tasks which I had to face. Originally, I had an idea of what kind of game I wanted to develop, gameplay wise
[ WARNING ] Enabled input prompt permutations. It means that generated results may vary on different steps. If it is not expected, please specify --disable_prompt_permutation in your benchmarking command to disable this behavior
[ INFO ] [warm-up] Batch_size=8, all input token size after padding: 119 * 8, all max_output_token_size: 128 * 8
[ INFO ] [warm-up][P0] Input token size: 952, Output size: 1024, Infer count: 128, Tokenization Time: 14.75ms, Detokenization Time: 1.36ms, Generation Time: 11.75s, Latency: 91.77 ms/8tokens
[ INFO ] [warm-up][P0] First token latency: 33426.61 ms/8tokens, other tokens latency: 59.58 ms/8tokens, len of tokens: 128 * 8
[ INFO ] [warm-up][P0] First infer latency: 4177.97 ms/infer, other infers latency: 57.99 ms/infer, inference count: 128
[ INFO ] [warm-up][P0] Result MD5:['e4d79dc955223d364e9d535a0b3679e2', 'e4d79dc955223d364e9d535a0b3679e2', 'd3deebe59b935f7ee07edb281aa1ad4d', 'e4d79dc955223d364e9d535a0b3679e2', 'e4d79dc955223d364e9d535a0b3679e2', 'e4d79dc955223d364e9d535a0b3679e2', 'e4d79dc955223d364e9d535a0b3679e2', 'e4d79dc955223d364e9d535a0b3679e2']
[ INFO ] [warm-up][P0] Generated: . I wanted to make a game where you have to survive, and you have to do it by eating. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to eat, and you have to do it by surviving. I
[ INFO ] [warm-up][P0] start: 2025-03-23T01:58:47.018681, end: 2025-03-23T01:58:58.784040
[ WARNING ] Enabled input prompt permutations. It means that generated results may vary on different steps. If it is not expected, please specify --disable_prompt_permutation in your benchmarking command to disable this behavior
[ INFO ] [1] Batch_size=8, all input token size after padding: 119 * 8, all max_output_token_size: 128 * 8
[ INFO ] [1][P0] Input token size: 952, Output size: 1024, Infer count: 128, Tokenization Time: 2.76ms, Detokenization Time: 1.16ms, Generation Time: 7.95s, Latency: 62.14 ms/8tokens
[ INFO ] [1][P0] First token latency: 4492.10 ms/8tokens, other tokens latency: 58.19 ms/8tokens, len of tokens: 128 * 8
[ INFO ] [1][P0] First infer latency: 561.40 ms/infer, other infers latency: 56.85 ms/infer, inference count: 128
[ INFO ] [1][P0] Result MD5:['d3deebe59b935f7ee07edb281aa1ad4d', '66eb5582b363c955eed954cc38f2373e', '54b93cc1fcb54cd655fafdd844d06900', '52650d6be5526740726debd2594482b8', 'f061013dc640f8f3b4ad8845170bf160', 'd3deebe59b935f7ee07edb281aa1ad4d', '54b93cc1fcb54cd655fafdd844d06900', 'd4bf3d5295455ee079d7dfaf0590de93']
[ INFO ] [1][P0] Generated: . I wanted to make a game where you have to survive, and you have to do it by eating. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to survive, and you have to do it by eating. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to survive, and you have to do it by eating. I wanted to make a game where you have to eat, and you have to do it by surviving. I
[ INFO ] [1][P0] start: 2025-03-23T01:58:58.784455, end: 2025-03-23T01:59:06.743554
[ WARNING ] Enabled input prompt permutations. It means that generated results may vary on different steps. If it is not expected, please specify --disable_prompt_permutation in your benchmarking command to disable this behavior
[ INFO ] [2] Batch_size=8, all input token size after padding: 119 * 8, all max_output_token_size: 128 * 8
[ INFO ] [2][P0] Input token size: 952, Output size: 1024, Infer count: 128, Tokenization Time: 2.39ms, Detokenization Time: 0.92ms, Generation Time: 7.85s, Latency: 61.34 ms/8tokens
[ INFO ] [2][P0] First token latency: 4415.10 ms/8tokens, other tokens latency: 57.46 ms/8tokens, len of tokens: 128 * 8
[ INFO ] [2][P0] First infer latency: 551.76 ms/infer, other infers latency: 56.20 ms/infer, inference count: 128
[ INFO ] [2][P0] Result MD5:['899740a2c5eddee8fd1f8354f266cd13', '4c0aaa4416da2858027e14ef6e9a0133', '4c0aaa4416da2858027e14ef6e9a0133', '8e6dbd00760e95b4a12ae702817e0152', 'e4d79dc955223d364e9d535a0b3679e2', '8e6dbd00760e95b4a12ae702817e0152', '8e6dbd00760e95b4a12ae702817e0152', '4c0aaa4416da2858027e14ef6e9a0133']
[ INFO ] [2][P0] Generated: . I wanted to make a game where you have to survive, and you can do that by eating. I wanted to make a game where you have to eat to survive, and you can do that by eating. I wanted to make a game where you have to eat to survive, and you can do that by eating. I wanted to make a game where you have to eat to survive, and you can do that by eating. I wanted to make a game where you have to eat to survive, and you can do that by eating. I wanted to make a game where you have to eat to survive, and you can do that
[ INFO ] [2][P0] start: 2025-03-23T01:59:06.743908, end: 2025-03-23T01:59:14.599684
[ WARNING ] Enabled input prompt permutations. It means that generated results may vary on different steps. If it is not expected, please specify --disable_prompt_permutation in your benchmarking command to disable this behavior
[ INFO ] [3] Batch_size=8, all input token size after padding: 119 * 8, all max_output_token_size: 128 * 8
[ INFO ] [3][P0] Input token size: 952, Output size: 1024, Infer count: 128, Tokenization Time: 1.85ms, Detokenization Time: 0.88ms, Generation Time: 8.44s, Latency: 65.96 ms/8tokens
[ INFO ] [3][P0] First token latency: 6816.69 ms/8tokens, other tokens latency: 59.75 ms/8tokens, len of tokens: 128 * 8
[ INFO ] [3][P0] First infer latency: 851.99 ms/infer, other infers latency: 58.69 ms/infer, inference count: 128
[ INFO ] [3][P0] Result MD5:['e4d79dc955223d364e9d535a0b3679e2', 'f061013dc640f8f3b4ad8845170bf160', '899740a2c5eddee8fd1f8354f266cd13', '899740a2c5eddee8fd1f8354f266cd13', '899740a2c5eddee8fd1f8354f266cd13', 'f061013dc640f8f3b4ad8845170bf160', 'f061013dc640f8f3b4ad8845170bf160', '899740a2c5eddee8fd1f8354f266cd13']
[ INFO ] [3][P0] Generated: . I wanted to make a game where you have to survive, and you have to do it by eating. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to eat, and you have to do it by surviving. I wanted to make a game where you have to eat, and you have to do it by surviving. I
[ INFO ] [3][P0] start: 2025-03-23T01:59:14.599989, end: 2025-03-23T01:59:23.046520
[ INFO ] <<< Warm-up iteration is excluded. >>>
[ INFO ] [Total] Iterations: 3
[ INFO ] [Average] P[0] Input token size: 952, 1st token latency: 5241.30 ms/8tokens, 2nd token latency: 58.47 ms/8tokens, 2nd tokens throughput: 136.83 tokens/s
/home/hoang-std/openvino_distributed/openvino.genai/tools/llm_bench/energy.py:10: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,88,89,90,91,96,97,98,99,104,105,106,107,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv("/home/hoang-std/pcm/build/bin/int8_llama-2-7b-chat-hf.csv")
Total energy: 10232.09
Power consumption: 568.4494444444445
